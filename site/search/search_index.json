{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Jianliang's GitHub page","title":"Home"},{"location":"#welcome-to-jianliangs-github-page","text":"","title":"Welcome to Jianliang's GitHub page"},{"location":"about/","text":"Affiliation Graduate School & Business School Imperial College London Title DR Others FHEA, IEEE member Nickname Liam Contact j.gao AT imperial dot ac dot uk","title":"About"},{"location":"about/#affiliation","text":"Graduate School & Business School Imperial College London","title":"Affiliation"},{"location":"about/#title","text":"DR","title":"Title"},{"location":"about/#others","text":"FHEA, IEEE member","title":"Others"},{"location":"about/#nickname","text":"Liam","title":"Nickname"},{"location":"about/#contact","text":"j.gao AT imperial dot ac dot uk","title":"Contact"},{"location":"log/","text":"Sharing is Great Loco mecum sunt ad meorum pendeat Lorem markdownum proles constitit, lignum tamen arce Aeneia, arbore nubes tua imbrem da intres . Hippocoon devexo inveniunt et Circen tibi illa Flentibus, et arbor prendere auctoremque sinuantur Phoebus. Curvique utque, quid tellus iterum interdictae cumque candida cura amata accepisse vincere. Illa undis nuntiet . De sedit o putes tamen sanguis in pacatum sinistra euntem faveas, in meritorum vana non dubitabat rexque murice. Posit It used to be known as RStudio. Oculos sub, ore antiqua colle ubi pugnantem adapertaque proles detractavitque est terra tuum mihi magni. Rabies omnia nocet, minister viri, undas praestare alma Saturnus nox a detrectas tenenti. Ea latebra commendat recusat abest corpora etiamnum praeter Pelethronium Salmacis; nunc sacra. Umentia famem nam quo auditque remos discrimine haec misit enixa ipsa ecce dorso conantem relictis potuit. At saxa inmunis et cunctos removerat adulantes nocte etiam census Nasamoniaci sed aetherium iamque adversa, utque? Sacra miliaque palmis lavere sibi, ubi ora modo infirmos, quodsi tum collocat vaticinor ruit quoque nam relatus tauri ? Testor celebrare funeris quinque, illi multo, unda ratione Scorpion infelix. Aliquid disiecit atras, sed non amans contigit, suumque ne obstitit supponas. Super tunc caede et auctor, virque novem cognovit vivis nec pectoris numen vicerat adest ipsa quondam, parens Hectorea. Concepit spectat delubraque pater inponis in omnia probabant finire domabile dixit per silvas ramos: metuis! Inmutat versus cum prima interea ferax.","title":"Sharing is Great"},{"location":"log/#sharing-is-great","text":"","title":"Sharing is Great"},{"location":"log/#loco-mecum-sunt-ad-meorum-pendeat","text":"Lorem markdownum proles constitit, lignum tamen arce Aeneia, arbore nubes tua imbrem da intres . Hippocoon devexo inveniunt et Circen tibi illa Flentibus, et arbor prendere auctoremque sinuantur Phoebus. Curvique utque, quid tellus iterum interdictae cumque candida cura amata accepisse vincere. Illa undis nuntiet . De sedit o putes tamen sanguis in pacatum sinistra euntem faveas, in meritorum vana non dubitabat rexque murice.","title":"Loco mecum sunt ad meorum pendeat"},{"location":"log/#posit","text":"It used to be known as RStudio. Oculos sub, ore antiqua colle ubi pugnantem adapertaque proles detractavitque est terra tuum mihi magni. Rabies omnia nocet, minister viri, undas praestare alma Saturnus nox a detrectas tenenti. Ea latebra commendat recusat abest corpora etiamnum praeter Pelethronium Salmacis; nunc sacra. Umentia famem nam quo auditque remos discrimine haec misit enixa ipsa ecce dorso conantem relictis potuit. At saxa inmunis et cunctos removerat adulantes nocte etiam census Nasamoniaci sed aetherium iamque adversa, utque? Sacra miliaque palmis lavere sibi, ubi ora modo infirmos, quodsi tum collocat vaticinor ruit quoque nam relatus tauri ? Testor celebrare funeris quinque, illi multo, unda ratione Scorpion infelix. Aliquid disiecit atras, sed non amans contigit, suumque ne obstitit supponas. Super tunc caede et auctor, virque novem cognovit vivis nec pectoris numen vicerat adest ipsa quondam, parens Hectorea. Concepit spectat delubraque pater inponis in omnia probabant finire domabile dixit per silvas ramos: metuis! Inmutat versus cum prima interea ferax.","title":"Posit"},{"location":"_posts/2017-07-21-PhenoMeNal-intro/","text":"Project Name PhenoMeNal -- Large Scale Computing for Medical Metabolomics PhenoMeNal (Phenome and Metabolome aNalysis) is a comprehensive and standardised e-infrastructure that supports the data processing and analysis pipelines for molecular phenotype data generated by metabolomics applications. Objectives To use existing open source community standards, integrate tools, resources and methods for the management, dissemination and computational analysis of very large datasets of human metabolic phenotyping and genomic data into a secure and sustainable e-Infrastructure. To operate and consolidate the PhenoMeNal e-infrastructure based on existing internal and external HPC (High-performance computing), cloud, and grid resources, including the EGI and the EGI Federated Cloud, and to extend it to world-wide computational infrastructures. To improve and scale-up tools used within the infrastructure to cope with very large datasets. To establish technology for a water-tight audit trail for the processing of human metabolic phenotyping data from the raw data acquisition all the way to the generation of high-level biomedical insights (such as a medical diagnosis). To establish privacy-protection methods that allow working with highly sensitive molecular phenotype data. To foster the worldwide adoption of PhenoMeNal through a wide range of outreach, dissemination, networking and training activities. To develop a model to ensure sustainability of the PhenoMeNal network. Website For more detail, please visit: Introduction https://www.youtube.com/watch?v=f8tmWikvAgw Tutorial https://www.ebi.ac.uk/training/online/courses/phenomenal-gateway/","title":"PhenoMeNal"},{"location":"_posts/2017-07-21-PhenoMeNal-intro/#project-name","text":"PhenoMeNal -- Large Scale Computing for Medical Metabolomics PhenoMeNal (Phenome and Metabolome aNalysis) is a comprehensive and standardised e-infrastructure that supports the data processing and analysis pipelines for molecular phenotype data generated by metabolomics applications.","title":"Project Name"},{"location":"_posts/2017-07-21-PhenoMeNal-intro/#objectives","text":"To use existing open source community standards, integrate tools, resources and methods for the management, dissemination and computational analysis of very large datasets of human metabolic phenotyping and genomic data into a secure and sustainable e-Infrastructure. To operate and consolidate the PhenoMeNal e-infrastructure based on existing internal and external HPC (High-performance computing), cloud, and grid resources, including the EGI and the EGI Federated Cloud, and to extend it to world-wide computational infrastructures. To improve and scale-up tools used within the infrastructure to cope with very large datasets. To establish technology for a water-tight audit trail for the processing of human metabolic phenotyping data from the raw data acquisition all the way to the generation of high-level biomedical insights (such as a medical diagnosis). To establish privacy-protection methods that allow working with highly sensitive molecular phenotype data. To foster the worldwide adoption of PhenoMeNal through a wide range of outreach, dissemination, networking and training activities. To develop a model to ensure sustainability of the PhenoMeNal network.","title":"Objectives"},{"location":"_posts/2017-07-21-PhenoMeNal-intro/#website","text":"For more detail, please visit: Introduction https://www.youtube.com/watch?v=f8tmWikvAgw Tutorial https://www.ebi.ac.uk/training/online/courses/phenomenal-gateway/","title":"Website"},{"location":"_posts/2017-07-31-KubeNow-on-Azure/","text":"Initialize KubeNow KubeNow provides one line command deployment of Kubernetes https://github.com/jianlianggao/KubeNow-plugin . Follow the intruction to download \"kn\", the main Kubernetes deployment bash script, and make it executable. It is not necessary to add it into system path. As long as you are comfortable and confident to run it as needed, you can decide wherever to put it. Once you successfully run kn --preset phenomenal init azure , you will see like: And, in the , you should be able see the files and directories listed as follows. Otherwise, there must be something wrong with your initialization. Please check your computer environment such as network connection, permission etc. Now it's time to create and retrieve the information of your subscription of Azure and modify the \"config.tfvars\" in the following lines. # Credentials subscription_id = \"your-subscription_id\" client_id = \"your-client_id\" # a.k.a. your appId client_secret = \"your-client_secret\" # a.k.a. password tenant_id = \"your-tenant_id\" To obtain your Azure account information, you can either check your Azure portal settings and properties https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal or run the \"az cli\" https://docs.microsoft.com/en-us/cli/azure/create-an-azure-service-principal-azure-cli?toc=%2Fen-us%2Fazure%2Fazure-resource-manager%2Ftoc.json&bc=%2Fen-us%2Fazure%2Fbread%2Ftoc.json&view=azure-cli-latest For example, we used \"az cli\" (Azure CLI. Installation of AZ CLI please check https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-apt?view=azure-cli-latest ) to create and retrieve our appId, tenant ID and confidential. (1) Run \"az login\" for the first time you will get a return code. Use the returned code to login in the link prompted. (2) After login successfully, you can run az ad sp create-for-rbac --role=\"Owner\" --scopes=\"/subscriptions/<your subscription ID>\" You should have return values including \"appId\", \"password\" and \"tenant\". Then you need to modify the following lines in the \"config.tfvars\" accoriding to your need and the design your Azure cluster. The type of nodes and VMs can be found at ( https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-general ) # Cluster configuration cluster_prefix = \"your-cluster-prefix\" # Your cluster prefix location = \"West Europe\" # Some location according to the the type of VMs you choose for your master, gluster and computing nodes. # Master configuration master_vm_size = \"Standard_DS2_v2\" # change this according to your need # Node configuration node_count = \"3\" # change the number of nodes according to your need node_vm_size = \"Standard_DS2_v2\" # change this according to your need # Gluster configuration glusternode_count = \"1\" # change the number of nodes according to the size of cluster glusternode_vm_size = \"Standard_DS2_v2\" # change this according to your need ..... # The following passwords need to be set with fairly complicated combination keys. \"action\" = { \"type\" = \"ansible-playbook\" \"playbook\" = \"plugins/phnmnl/KubeNow-plugin/playbooks/install-phenomenal-playbook.yml\" \"extra_vars\" = { \"galaxy_include\" = \"true\" \"galaxy_admin_email\" = \"yoourname@bla.bla.com\" \"galaxy_admin_password\" = \"YVeciekOpe!@\" \"jupyter_include\" = \"true\" \"jupyter_password\" = \"VNidieKOEy#!\" \"luigi_include\" = \"true\" \"luigi_username\" = \"admin\" \"luigi_password\" = \"reTEiopLnuuQ\" \"dashboard_include\" = \"false\" \"dashboard_username\" = \"admin\" \"dashboard_password\" = \"@\u00a3$556YuendK\" } }","title":"Kubernetes on Azure"},{"location":"_posts/2017-07-31-KubeNow-on-Azure/#initialize-kubenow","text":"KubeNow provides one line command deployment of Kubernetes https://github.com/jianlianggao/KubeNow-plugin . Follow the intruction to download \"kn\", the main Kubernetes deployment bash script, and make it executable. It is not necessary to add it into system path. As long as you are comfortable and confident to run it as needed, you can decide wherever to put it. Once you successfully run kn --preset phenomenal init azure , you will see like: And, in the , you should be able see the files and directories listed as follows. Otherwise, there must be something wrong with your initialization. Please check your computer environment such as network connection, permission etc. Now it's time to create and retrieve the information of your subscription of Azure and modify the \"config.tfvars\" in the following lines. # Credentials subscription_id = \"your-subscription_id\" client_id = \"your-client_id\" # a.k.a. your appId client_secret = \"your-client_secret\" # a.k.a. password tenant_id = \"your-tenant_id\" To obtain your Azure account information, you can either check your Azure portal settings and properties https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal or run the \"az cli\" https://docs.microsoft.com/en-us/cli/azure/create-an-azure-service-principal-azure-cli?toc=%2Fen-us%2Fazure%2Fazure-resource-manager%2Ftoc.json&bc=%2Fen-us%2Fazure%2Fbread%2Ftoc.json&view=azure-cli-latest For example, we used \"az cli\" (Azure CLI. Installation of AZ CLI please check https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-apt?view=azure-cli-latest ) to create and retrieve our appId, tenant ID and confidential. (1) Run \"az login\" for the first time you will get a return code. Use the returned code to login in the link prompted. (2) After login successfully, you can run az ad sp create-for-rbac --role=\"Owner\" --scopes=\"/subscriptions/<your subscription ID>\" You should have return values including \"appId\", \"password\" and \"tenant\". Then you need to modify the following lines in the \"config.tfvars\" accoriding to your need and the design your Azure cluster. The type of nodes and VMs can be found at ( https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-general ) # Cluster configuration cluster_prefix = \"your-cluster-prefix\" # Your cluster prefix location = \"West Europe\" # Some location according to the the type of VMs you choose for your master, gluster and computing nodes. # Master configuration master_vm_size = \"Standard_DS2_v2\" # change this according to your need # Node configuration node_count = \"3\" # change the number of nodes according to your need node_vm_size = \"Standard_DS2_v2\" # change this according to your need # Gluster configuration glusternode_count = \"1\" # change the number of nodes according to the size of cluster glusternode_vm_size = \"Standard_DS2_v2\" # change this according to your need ..... # The following passwords need to be set with fairly complicated combination keys. \"action\" = { \"type\" = \"ansible-playbook\" \"playbook\" = \"plugins/phnmnl/KubeNow-plugin/playbooks/install-phenomenal-playbook.yml\" \"extra_vars\" = { \"galaxy_include\" = \"true\" \"galaxy_admin_email\" = \"yoourname@bla.bla.com\" \"galaxy_admin_password\" = \"YVeciekOpe!@\" \"jupyter_include\" = \"true\" \"jupyter_password\" = \"VNidieKOEy#!\" \"luigi_include\" = \"true\" \"luigi_username\" = \"admin\" \"luigi_password\" = \"reTEiopLnuuQ\" \"dashboard_include\" = \"false\" \"dashboard_username\" = \"admin\" \"dashboard_password\" = \"@\u00a3$556YuendK\" } }","title":"Initialize KubeNow"},{"location":"_posts/2017-08-04-KubeNow-Demo-Azure/","text":"Luigi and Jupyter notebook demo on Azure. We have kept a series of screen recording of KubeNow deployment on Azure. (1) Deployment of KubeNow with Galaxy, Jupyter notebook and Luigi. The length of this video is about 30 minutes. You don't need to follow it all the way through, otherwise you will feel bored. https://www.youtube.com/watch?v=nTfClmIwe8Y (2) Access to deployed Jupyter notebook. https://www.youtube.com/watch?v=k1R1gNPl1ZA (3) Interface of Jupyter notebook. https://www.youtube.com/watch?v=UmJUH1TmJb4 (4) Luigi GUI associated with Jupyter notebook. https://www.youtube.com/watch?v=I84HlDMCF_c The demo didn't run properly as a setting for Luigi wasn't correct. (5) PAPY demo with correct Luigi settings. https://www.youtube.com/watch?v=J46Ljl4KYeU and https://www.youtube.com/watch?v=wNqt4kABgTk Galaxy instance demo on Azure This is a demo for Galaxy instance by running PAPY and BATMAN tools.","title":"KubeNow deployment Demo on Azure"},{"location":"_posts/2018-01-21-dHCP-intro/","text":"Project Name dCHP -- The Developing Human Connectome Project The Developing Human Connectome Project (dHCP), led by King\u2019s College London, Imperial College London and Oxford University, aims to make major scientific progress by creating the first 4-dimensional connectome of early life. Our goal is to create a dynamic map of human brain connectivity from 20 to 44 weeks post-conceptional age, which will link together imaging, clinical, behavioural, and genetic information. This unique setting, with imaging and collateral data in an expandable open-source informatics structure, will permit wide use by the scientific community, and to undertake pioneer studies into normal and abnormal development by studying well-phenotyped and genotyped group of infants with specific genetic and environmental risks that could lead to Autistic Spectrum Disorder or Cerebral Palsy. Website For more detail, please visit: http://www.developingconnectome.org","title":"dHCP"},{"location":"_posts/2018-01-21-dHCP-intro/#project-name","text":"dCHP -- The Developing Human Connectome Project The Developing Human Connectome Project (dHCP), led by King\u2019s College London, Imperial College London and Oxford University, aims to make major scientific progress by creating the first 4-dimensional connectome of early life. Our goal is to create a dynamic map of human brain connectivity from 20 to 44 weeks post-conceptional age, which will link together imaging, clinical, behavioural, and genetic information. This unique setting, with imaging and collateral data in an expandable open-source informatics structure, will permit wide use by the scientific community, and to undertake pioneer studies into normal and abnormal development by studying well-phenotyped and genotyped group of infants with specific genetic and environmental risks that could lead to Autistic Spectrum Disorder or Cerebral Palsy.","title":"Project Name"},{"location":"_posts/2018-01-21-dHCP-intro/#website","text":"For more detail, please visit: http://www.developingconnectome.org","title":"Website"},{"location":"_posts/2020-02-28-Galaxy-local-deploy-docker-tool/","text":"Galaxy local deployment was tested on my local computer with Ubuntu 16.04 LTS and remote VM with Ubuntu 18.04 LTS. The following tutorial is based on Ubuntu 18.04.4 LTS (bionic). Step 1. Preparation your local system Firstly, you need to have your system ready. Have Docker-ce installed and configured on your system. My tool was encapsulated in a Docker image, and my local Galaxy instance aimed to run my tool with Docker engine. Therefore I needed Docker daemon running on my local system before I launched Galaxy instance. Following instructions from Digital Ocean ( https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04 ) to install Docker-ce on your system. Here I simply duplicate the key steps. sudo apt update && sudo apt install apt-transport-https ca-certificates curl software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\" && sudo apt update sudo apt install docker-ce sudo usermod -aG docker ${USER} ( NOTE: immediately after this command, you can run \"docker images\" in your current user (not root user) to check if your user has permissions to run docker. If you have such error \"Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock:\" , you need to logout and login again to re-check. If you still don't have permission to run docker, use the following command to add your user into docker group sudo gpasswd -a ubuntu docker . Logout and login are needed immediately after the above command. You may also need to run sudo systemctl restart docker after login again.) To insure Galaxy instance can run docker with galaxy users, you need to modify the file /etc/sudoers and add the following line into it. galaxy ALL = (root) NOPASSWD: SETENV: /usr/bin/docker Your system is now ready to deploy Galaxy. Step 2. Clone Galaxy project and deploy locally. 1. git clone Galaxy from Galaxy project GitHub https://github.com/galaxyproject/galaxy.git ( NOTE: I tested on 2 ext4 partitions and 1 ntfs partition. The test on ntfs partition failed) 2. Enter galaxy directory and copy config/galaxy.yml.sample as config/galaxy.yml . 3. In galaxy direcotry, copy config/job_conf.xml.sample_basic as config/job_conf.xml and edit the configuration file as follows. <?xml version=\"1.0\"?> <!-- A sample job config that explicitly configures job running the way it is configured by default (if there is no explicit config). --> <job_conf> <plugins> <plugin id=\"local\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\" workers=\"1\"/> </plugins> <destinations default=\"local\"> <destination id=\"local\" runner=\"local\"/> <destination id=\"docker_local\" runner=\"local\"> <param id=\"docker_enabled\">true</param> </destination> <destination id=\"<your tool id>-container\" runner=\"local\"> <param id=\"docker_enabled\">true</param> <param id=\"docker_volumes\">$defaults</param> --> <param id=\"docker_sudo\">false</param> <param id=\"docker_auto_rm\">true</param> </destination> </destinations> <tools> <tool id=\"<your tool id>\" destination=\"<your tool id>-container\"/> </tools> </job_conf> 4. Edit config/tool_conf.xml by adding your tool, which allows Galay to list your tool on the left-hand side panel. For example, <?xml version=\"1.0\"?> <!-- other tools --> <section id=\"<your tool>-tool\" name=\"Name of your tool\"> <tool file=\"<path to>/<your tool>.xml\" /> NOTE: path to your tool needs to be subdirectory in galaxy/tools . For example, galaxy/tools/<path to>/<your tool>.xml . 5. Create your tool XML file galaxy/tools/<path to>/<your tool>.xml , containing the following content as running command lines, input parameters, output parameters and help messages etc. <tool id=\"<your tool id>\" name=\"<Tool name> (in docker)\"> <description>Metabo Flow</description> <requirements> <container type=\"docker\">docker_hub_id/tool_docker_name:tag</container> </requirements> <command><![CDATA[ <entrypoint> <inputs ${testdata_input} ${pre_define_sd}> -o outputs ]]> </command> <inputs> <param name=\"testdata_input\" type=\"data\" label=\"Test Dataset\" help=\".\" /> <param name=\"pre_define_sd\" type=\"data\" label=\"Pre-defined standard deviation matrix\" /> </inputs> <outputs> <data name=\"data_long\" format=\"csv\" from_work_dir=\"outputs/data_long.csv\" label=\"data_long.csv\"/> <data name=\"data_wide\" format=\"csv\" from_work_dir=\"outputs/data_wide.csv\" label=\"data_wide.csv\"/> <data name=\"data_wide_Symb\" format=\"csv\" from_work_dir=\"outputs/data_wide_Symb.csv\" label=\"data_wide_Symb.csv\"/> <data name=\"stats_median_batch_corrected_data\" format=\"txt\" from_work_dir=\"outputs/stats_median_batch_corrected_data.txt\" label=\"stats_median_batch_corrected_data.txt\"/> <data name=\"stats_outliers\" format=\"txt\" from_work_dir=\"outputs/stats_outliers.txt\" label=\"stats_outliers.txt\"/> <data name=\"stats_raw_data\" format=\"txt\" from_work_dir=\"outputs/stats_raw_data.txt\" label=\"stats_raw_data.txt\"/> <data name=\"stats_standardised_data\" format=\"txt\" from_work_dir=\"outputs/stats_standardised_data.txt\" label=\"stats_standardised_data.txt\"/> <data name=\"plot_logConcentration_by_batch\" format=\"pdf\" from_work_dir=\"outputs/plot_logConcentration_by_batch.pdf\" label=\"plot_logConcentration_by_batch.pdf\"/> <data name=\"plot_logConcentration_z_scores\" format=\"pdf\" from_work_dir=\"outputs/plot_logConcentration_z_scores.pdf\" label=\"plot_logConcentration_z_scores.pdf\"/> </outputs> <help> </help> </tool> Step 3. Run deployment command sh run.sh in galaxy directory. NOTE: If you deploy Galaxy instance on a VM on cluster or cloud, you may need a reverse proxy to access your Galaxy instance from a remote web browser, because by default settings, Galaxy instance listens to localhost (127.0.0.1) at port 8080.","title":"Galaxy local Dev"},{"location":"_posts/2020-02-28-Galaxy-local-deploy-docker-tool/#step-1-preparation-your-local-system","text":"Firstly, you need to have your system ready.","title":"Step 1. Preparation your local system"},{"location":"_posts/2020-02-28-Galaxy-local-deploy-docker-tool/#have-docker-ce-installed-and-configured-on-your-system","text":"My tool was encapsulated in a Docker image, and my local Galaxy instance aimed to run my tool with Docker engine. Therefore I needed Docker daemon running on my local system before I launched Galaxy instance. Following instructions from Digital Ocean ( https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04 ) to install Docker-ce on your system. Here I simply duplicate the key steps. sudo apt update && sudo apt install apt-transport-https ca-certificates curl software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\" && sudo apt update sudo apt install docker-ce sudo usermod -aG docker ${USER} ( NOTE: immediately after this command, you can run \"docker images\" in your current user (not root user) to check if your user has permissions to run docker. If you have such error \"Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock:\" , you need to logout and login again to re-check. If you still don't have permission to run docker, use the following command to add your user into docker group sudo gpasswd -a ubuntu docker . Logout and login are needed immediately after the above command. You may also need to run sudo systemctl restart docker after login again.) To insure Galaxy instance can run docker with galaxy users, you need to modify the file /etc/sudoers and add the following line into it. galaxy ALL = (root) NOPASSWD: SETENV: /usr/bin/docker Your system is now ready to deploy Galaxy.","title":"Have Docker-ce installed and configured on your system."},{"location":"_posts/2020-02-28-Galaxy-local-deploy-docker-tool/#step-2-clone-galaxy-project-and-deploy-locally","text":"1. git clone Galaxy from Galaxy project GitHub https://github.com/galaxyproject/galaxy.git ( NOTE: I tested on 2 ext4 partitions and 1 ntfs partition. The test on ntfs partition failed) 2. Enter galaxy directory and copy config/galaxy.yml.sample as config/galaxy.yml . 3. In galaxy direcotry, copy config/job_conf.xml.sample_basic as config/job_conf.xml and edit the configuration file as follows. <?xml version=\"1.0\"?> <!-- A sample job config that explicitly configures job running the way it is configured by default (if there is no explicit config). --> <job_conf> <plugins> <plugin id=\"local\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\" workers=\"1\"/> </plugins> <destinations default=\"local\"> <destination id=\"local\" runner=\"local\"/> <destination id=\"docker_local\" runner=\"local\"> <param id=\"docker_enabled\">true</param> </destination> <destination id=\"<your tool id>-container\" runner=\"local\"> <param id=\"docker_enabled\">true</param> <param id=\"docker_volumes\">$defaults</param> --> <param id=\"docker_sudo\">false</param> <param id=\"docker_auto_rm\">true</param> </destination> </destinations> <tools> <tool id=\"<your tool id>\" destination=\"<your tool id>-container\"/> </tools> </job_conf> 4. Edit config/tool_conf.xml by adding your tool, which allows Galay to list your tool on the left-hand side panel. For example, <?xml version=\"1.0\"?> <!-- other tools --> <section id=\"<your tool>-tool\" name=\"Name of your tool\"> <tool file=\"<path to>/<your tool>.xml\" /> NOTE: path to your tool needs to be subdirectory in galaxy/tools . For example, galaxy/tools/<path to>/<your tool>.xml . 5. Create your tool XML file galaxy/tools/<path to>/<your tool>.xml , containing the following content as running command lines, input parameters, output parameters and help messages etc. <tool id=\"<your tool id>\" name=\"<Tool name> (in docker)\"> <description>Metabo Flow</description> <requirements> <container type=\"docker\">docker_hub_id/tool_docker_name:tag</container> </requirements> <command><![CDATA[ <entrypoint> <inputs ${testdata_input} ${pre_define_sd}> -o outputs ]]> </command> <inputs> <param name=\"testdata_input\" type=\"data\" label=\"Test Dataset\" help=\".\" /> <param name=\"pre_define_sd\" type=\"data\" label=\"Pre-defined standard deviation matrix\" /> </inputs> <outputs> <data name=\"data_long\" format=\"csv\" from_work_dir=\"outputs/data_long.csv\" label=\"data_long.csv\"/> <data name=\"data_wide\" format=\"csv\" from_work_dir=\"outputs/data_wide.csv\" label=\"data_wide.csv\"/> <data name=\"data_wide_Symb\" format=\"csv\" from_work_dir=\"outputs/data_wide_Symb.csv\" label=\"data_wide_Symb.csv\"/> <data name=\"stats_median_batch_corrected_data\" format=\"txt\" from_work_dir=\"outputs/stats_median_batch_corrected_data.txt\" label=\"stats_median_batch_corrected_data.txt\"/> <data name=\"stats_outliers\" format=\"txt\" from_work_dir=\"outputs/stats_outliers.txt\" label=\"stats_outliers.txt\"/> <data name=\"stats_raw_data\" format=\"txt\" from_work_dir=\"outputs/stats_raw_data.txt\" label=\"stats_raw_data.txt\"/> <data name=\"stats_standardised_data\" format=\"txt\" from_work_dir=\"outputs/stats_standardised_data.txt\" label=\"stats_standardised_data.txt\"/> <data name=\"plot_logConcentration_by_batch\" format=\"pdf\" from_work_dir=\"outputs/plot_logConcentration_by_batch.pdf\" label=\"plot_logConcentration_by_batch.pdf\"/> <data name=\"plot_logConcentration_z_scores\" format=\"pdf\" from_work_dir=\"outputs/plot_logConcentration_z_scores.pdf\" label=\"plot_logConcentration_z_scores.pdf\"/> </outputs> <help> </help> </tool>","title":"Step 2. Clone Galaxy project and deploy locally."},{"location":"_posts/2020-02-28-Galaxy-local-deploy-docker-tool/#step-3-run-deployment-command-sh-runsh-in-galaxy-directory","text":"NOTE: If you deploy Galaxy instance on a VM on cluster or cloud, you may need a reverse proxy to access your Galaxy instance from a remote web browser, because by default settings, Galaxy instance listens to localhost (127.0.0.1) at port 8080.","title":"Step 3. Run deployment command sh run.sh in galaxy directory."},{"location":"_posts/2020-04-07-psql-query-performance/","text":"This is a glimpse of PostgreSQL queries performance based-on some basic practice on DELETE queries. The tests of the queries were implemented in Python and run on a PostgreSQL database installed on a VM (2 Intel Xeon CPU E5-2690 v3 @ 2.60GHz CPUs, 4GB RAM, Ubuntu 18.04 LTS bionic). A Python script for deleting items from tables. To completely delete a user record in XNAT database, items from 6 tables need to be deleted. At the beginning, I manually ran DELETE queries in a terminal connecting to the PostgreSQL database. Then I started thinking if I could run the 6 DELETE queries in one go to delete each inactive user, my job would be much easier. After a bit research, a Python library named psycopg2 was found to satisfy my job. A Python script was immediately written as import psycopg2, sys args = sys.argv if (len(args) <3 ): print('user_id and user_email are required!') exit(0) user_id = sys.argv[1] user_email = sys.argv[2] # 6 SQL quueries need to be done for delete a user from the XNAT db del1 = \"delete from xdat_user where login='\"+user_id+\"'\" del2 = \"delete from xhbm_user_registration_data where login='\"+user_id+\"'\" del3 = \"delete from xs_item_cache where contents like '%\"+user_id+\"%' AND contents like '%\"+user_email+\"%'\" del4 = \"delete from xhbm_xdat_user_auth where auth_user='\"+user_id+\"'\" del5 = \"delete from xhbm_alias_token where xdat_user_id='\"+user_id+\"'\" del6 = \"delete from xdat_user_history where login='\"+user_id+\"'\" try: connection = psycopg2.connect(host='my host ip', database='xnat', user=<user>, password=<password>, port=<port number>) cursor = connection.cursor() cursor.execute(del1) connection.commit() print('1. Deleted from xdat_user') cursor.execute(del2) connection.commit() print('2. Deleted from xhbm_user_registration_data') cursor.execute(del3) connection.commit() print('3. Deleted from xs_item_cache') cursor.execute(del4) connection.commit() print('4. Deleted from xhbm_xdat_user_auth') cursor.execute(del5) connection.commit() print('5. Deleted from xhbm_alias_token') cursor.execute(del6) connection.commit() print('6. Deleted from xdat_user_history') except (Exception, psycopg2.Error) as error : print (\"Error while getting data from PostgreSQL\", error) finally: if(connection): cursor.close() connection.close() print('db connection closed.') After a few runs, I had an idea. I wanted to observe how much differece between 6 queries within one db connection and 6 queries in 6 separate db connection. Then I did 107 runs with the script above, and 50 other runs with the script below. import psycopg2, sys args = sys.argv if (len(args) <3 ): print('user_id and user_email are required!') exit(0) user_id = sys.argv[1] user_email = sys.argv[2] # 6 SQL quueries need to be done for delete a user from the XNAT db del1 = \"delete from xdat_user where login='\"+user_id+\"'\" del2 = \"delete from xhbm_user_registration_data where login='\"+user_id+\"'\" del3 = \"delete from xs_item_cache where contents like '%\"+user_id+\"%' AND contents like '%\"+user_email+\"%'\" del4 = \"delete from xhbm_xdat_user_auth where auth_user='\"+user_id+\"'\" del5 = \"delete from xhbm_alias_token where xdat_user_id='\"+user_id+\"'\" del6 = \"delete from xdat_user_history where login='\"+user_id+\"'\" try: connection = psycopg2.connect(host='my host ip', database='xnat', user=<user>, password=<password>, port=<port number>) cursor = connection.cursor() cursor.execute(del1) connection.commit() except (Exception, psycopg2.Error) as error : print (\"Error while getting data from PostgreSQL\", error) finally: if(connection): cursor.close() connection.close() print('1. Deleted from xdat_user') try: connection = psycopg2.connect(host='my host ip', database='xnat', user=<user>, password=<password>, port=<port number>) cursor = connection.cursor() cursor.execute(del2) connection.commit() except (Exception, psycopg2.Error) as error : print (\"Error while getting data from PostgreSQL\", error) finally: if(connection): cursor.close() connection.close() print('2. Deleted from xhbm_user_registration_data') try: connection = psycopg2.connect(host='my host ip', database='xnat', user=<user>, password=<password>, port=<port number>) cursor = connection.cursor() cursor.execute(del3) connection.commit() except (Exception, psycopg2.Error) as error : print (\"Error while getting data from PostgreSQL\", error) finally: if(connection): cursor.close() connection.close() print('3. Deleted from xs_item_cache') try: connection = psycopg2.connect(host='my host ip', database='xnat', user=<user>, password=<password>, port=<port number>) cursor = connection.cursor() cursor.execute(del4) connection.commit() except (Exception, psycopg2.Error) as error : print (\"Error while getting data from PostgreSQL\", error) finally: if(connection): cursor.close() connection.close() print('4. Deleted from xhbm_xdat_user_auth') try: connection = psycopg2.connect(host='my host ip', database='xnat', user=<user>, password=<password>, port=<port number>) cursor = connection.cursor() cursor.execute(del5) connection.commit() except (Exception, psycopg2.Error) as error : print (\"Error while getting data from PostgreSQL\", error) finally: if(connection): cursor.close() connection.close() print('5. Deleted from xhbm_alias_token') try: connection = psycopg2.connect(host='my host ip', database='xnat', user=<user>, password=<password>, port=<port number>) cursor = connection.cursor() cursor.execute(del6) connection.commit() except (Exception, psycopg2.Error) as error : print (\"Error while getting data from PostgreSQL\", error) finally: if(connection): cursor.close() connection.close() print('6. Deleted from xdat_user_history') ANOVA ( from statsmodels.formula.api import ols ) had output as df sum_sq mean_sq F PR(>F) Queries 1.0 0.043661 0.043661 33.267452 4.226861e-08 Residual 155.0 0.203425 0.001312 NaN NaN It is an easy conclusion because the p-value (4.226861e-08) < 0.05 suggested to reject the null hypothesis. The mean time of 6 separate db connections is about 10% slower than that of single db connection.","title":"PostgreSQL queries performance"},{"location":"_posts/2020-04-07-psql-query-performance/#a-python-script-for-deleting-items-from-tables","text":"To completely delete a user record in XNAT database, items from 6 tables need to be deleted. At the beginning, I manually ran DELETE queries in a terminal connecting to the PostgreSQL database. Then I started thinking if I could run the 6 DELETE queries in one go to delete each inactive user, my job would be much easier. After a bit research, a Python library named psycopg2 was found to satisfy my job. A Python script was immediately written as import psycopg2, sys args = sys.argv if (len(args) <3 ): print('user_id and user_email are required!') exit(0) user_id = sys.argv[1] user_email = sys.argv[2] # 6 SQL quueries need to be done for delete a user from the XNAT db del1 = \"delete from xdat_user where login='\"+user_id+\"'\" del2 = \"delete from xhbm_user_registration_data where login='\"+user_id+\"'\" del3 = \"delete from xs_item_cache where contents like '%\"+user_id+\"%' AND contents like '%\"+user_email+\"%'\" del4 = \"delete from xhbm_xdat_user_auth where auth_user='\"+user_id+\"'\" del5 = \"delete from xhbm_alias_token where xdat_user_id='\"+user_id+\"'\" del6 = \"delete from xdat_user_history where login='\"+user_id+\"'\" try: connection = psycopg2.connect(host='my host ip', database='xnat', user=<user>, password=<password>, port=<port number>) cursor = connection.cursor() cursor.execute(del1) connection.commit() print('1. Deleted from xdat_user') cursor.execute(del2) connection.commit() print('2. Deleted from xhbm_user_registration_data') cursor.execute(del3) connection.commit() print('3. Deleted from xs_item_cache') cursor.execute(del4) connection.commit() print('4. Deleted from xhbm_xdat_user_auth') cursor.execute(del5) connection.commit() print('5. Deleted from xhbm_alias_token') cursor.execute(del6) connection.commit() print('6. Deleted from xdat_user_history') except (Exception, psycopg2.Error) as error : print (\"Error while getting data from PostgreSQL\", error) finally: if(connection): cursor.close() connection.close() print('db connection closed.') After a few runs, I had an idea. I wanted to observe how much differece between 6 queries within one db connection and 6 queries in 6 separate db connection. Then I did 107 runs with the script above, and 50 other runs with the script below. import psycopg2, sys args = sys.argv if (len(args) <3 ): print('user_id and user_email are required!') exit(0) user_id = sys.argv[1] user_email = sys.argv[2] # 6 SQL quueries need to be done for delete a user from the XNAT db del1 = \"delete from xdat_user where login='\"+user_id+\"'\" del2 = \"delete from xhbm_user_registration_data where login='\"+user_id+\"'\" del3 = \"delete from xs_item_cache where contents like '%\"+user_id+\"%' AND contents like '%\"+user_email+\"%'\" del4 = \"delete from xhbm_xdat_user_auth where auth_user='\"+user_id+\"'\" del5 = \"delete from xhbm_alias_token where xdat_user_id='\"+user_id+\"'\" del6 = \"delete from xdat_user_history where login='\"+user_id+\"'\" try: connection = psycopg2.connect(host='my host ip', database='xnat', user=<user>, password=<password>, port=<port number>) cursor = connection.cursor() cursor.execute(del1) connection.commit() except (Exception, psycopg2.Error) as error : print (\"Error while getting data from PostgreSQL\", error) finally: if(connection): cursor.close() connection.close() print('1. Deleted from xdat_user') try: connection = psycopg2.connect(host='my host ip', database='xnat', user=<user>, password=<password>, port=<port number>) cursor = connection.cursor() cursor.execute(del2) connection.commit() except (Exception, psycopg2.Error) as error : print (\"Error while getting data from PostgreSQL\", error) finally: if(connection): cursor.close() connection.close() print('2. Deleted from xhbm_user_registration_data') try: connection = psycopg2.connect(host='my host ip', database='xnat', user=<user>, password=<password>, port=<port number>) cursor = connection.cursor() cursor.execute(del3) connection.commit() except (Exception, psycopg2.Error) as error : print (\"Error while getting data from PostgreSQL\", error) finally: if(connection): cursor.close() connection.close() print('3. Deleted from xs_item_cache') try: connection = psycopg2.connect(host='my host ip', database='xnat', user=<user>, password=<password>, port=<port number>) cursor = connection.cursor() cursor.execute(del4) connection.commit() except (Exception, psycopg2.Error) as error : print (\"Error while getting data from PostgreSQL\", error) finally: if(connection): cursor.close() connection.close() print('4. Deleted from xhbm_xdat_user_auth') try: connection = psycopg2.connect(host='my host ip', database='xnat', user=<user>, password=<password>, port=<port number>) cursor = connection.cursor() cursor.execute(del5) connection.commit() except (Exception, psycopg2.Error) as error : print (\"Error while getting data from PostgreSQL\", error) finally: if(connection): cursor.close() connection.close() print('5. Deleted from xhbm_alias_token') try: connection = psycopg2.connect(host='my host ip', database='xnat', user=<user>, password=<password>, port=<port number>) cursor = connection.cursor() cursor.execute(del6) connection.commit() except (Exception, psycopg2.Error) as error : print (\"Error while getting data from PostgreSQL\", error) finally: if(connection): cursor.close() connection.close() print('6. Deleted from xdat_user_history') ANOVA ( from statsmodels.formula.api import ols ) had output as df sum_sq mean_sq F PR(>F) Queries 1.0 0.043661 0.043661 33.267452 4.226861e-08 Residual 155.0 0.203425 0.001312 NaN NaN It is an easy conclusion because the p-value (4.226861e-08) < 0.05 suggested to reject the null hypothesis. The mean time of 6 separate db connections is about 10% slower than that of single db connection.","title":"A Python script for deleting items from tables."}]}